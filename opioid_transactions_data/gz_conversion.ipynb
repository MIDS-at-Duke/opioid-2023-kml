{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "# with gzip.open('01_gz_files/arcos-fl-statewide-itemized.csv.gz', 'rt', newline='') as csv_file:\n",
    "#     csv_data = csv_file.read()\n",
    "#     with open('florida.csv', 'wt') as out_file:\n",
    "#          out_file.write(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modify state and state abbreviation\n",
    "state_name = 'ohio'\n",
    "state_abv = 'OH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chunk CSV File \n",
    "csv_file_name = '02_raw_data/' + state_name + '.csv'\n",
    "chunk_size = 25000\n",
    "keep_cols = ['BUYER_COUNTY', 'BUYER_STATE', 'TRANSACTION_DATE', 'CALC_BASE_WT_IN_GM', 'MME_Conversion_Factor']\n",
    "filtered_data = []\n",
    "state = [state_abv]\n",
    "\n",
    "with open(csv_file_name, 'r') as csv_file:\n",
    "    # Read the CSV file in chunks\n",
    "    csv_reader = pd.read_csv(csv_file, chunksize=chunk_size, usecols=keep_cols, low_memory=False)\n",
    "\n",
    "    # Iterate through chunks of the CSV file\n",
    "    for i, chunk in enumerate(csv_reader):\n",
    "        # Filter data for specific states\n",
    "        mod_chunk = chunk.loc[chunk['BUYER_STATE'].isin(state)]\n",
    "        \n",
    "        # Convert TRANSACTION_DATE to string type\n",
    "        mod_chunk['TRANSACTION_DATE'] = mod_chunk['TRANSACTION_DATE'].astype(str)\n",
    "\n",
    "        try:\n",
    "            # Split the date column and convert to integers\n",
    "            mod_chunk[['TransactionYear', 'TransactionMonth', 'TransactionDay']] = mod_chunk['TRANSACTION_DATE'].str.split('-', expand=True).astype(int)\n",
    "        except Exception as e:\n",
    "            # Handle the error by inserting NaN values for all three date columns\n",
    "            mod_chunk[['TransactionYear', 'TransactionMonth', 'TransactionDay']] = pd.NA\n",
    "\n",
    "        # Drop the original 'TRANSACTION_DATE' and 'TransactionDay' columns\n",
    "        mod_chunk.drop(['TRANSACTION_DATE', 'TransactionDay'], axis=1, inplace=True)\n",
    "\n",
    "        # Keep data within the specified year range\n",
    "        append_chunk = mod_chunk[(mod_chunk['TransactionYear'] >= 2003) & (mod_chunk['TransactionYear'] <= 2015)]\n",
    "        \n",
    "        # Append the processed chunk to the list\n",
    "        filtered_data.append(append_chunk)\n",
    "\n",
    "# Concatenate all processed chunks into a single DataFrame\n",
    "selected_data = pd.concat(filtered_data, ignore_index=True)\n",
    "\n",
    "# Specify the path where you want to save the Parquet file\n",
    "parquet_file_path = '03_chunked/' + state_name + '.parquet'\n",
    "\n",
    "# Convert the Pandas DataFrame to a PyArrow Table\n",
    "table = pa.Table.from_pandas(selected_data)\n",
    "\n",
    "# Write the Table to a Parquet file\n",
    "pq.write_table(table, parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         MME_Conversion_Factor  CALC_BASE_WT_IN_GM BUYER_STATE BUYER_COUNTY  \\\n",
      "9180444                    1.0             1.21080          OH         ERIE   \n",
      "7525272                    1.0             1.51350          OH        STARK   \n",
      "3353505                    1.5             2.24125          OH     HAMILTON   \n",
      "3510340                    1.5             5.37900          OH        LUCAS   \n",
      "9897016                    1.5             7.17200          OH      BELMONT   \n",
      "\n",
      "         TransactionYear  TransactionMonth  morphine_equivalent_g  \n",
      "9180444             2007                 8               1.210800  \n",
      "7525272             2009                12               1.513500  \n",
      "3353505             2012                 6               3.361875  \n",
      "3510340             2009                12               8.068500  \n",
      "9897016             2009                 5              10.758000  \n",
      "    BUYER_STATE BUYER_COUNTY  TransactionYear  MME_Conversion_Factor  \\\n",
      "813          OH       VINTON             2009                  661.0   \n",
      "125          OH     CLERMONT             2011                34372.5   \n",
      "814          OH       VINTON             2010                  806.0   \n",
      "236          OH      FAYETTE             2012                 4480.0   \n",
      "562          OH   MONTGOMERY             2008                61573.5   \n",
      "\n",
      "     CALC_BASE_WT_IN_GM  morphine_equivalent_g  \n",
      "813         1240.634600            1561.581600  \n",
      "125        75962.835644          101551.390063  \n",
      "814         1247.320625            1634.496562  \n",
      "236         9942.841393           13334.629706  \n",
      "562       139678.219270          177551.251059  \n"
     ]
    }
   ],
   "source": [
    "# Read opioid transaction data into pandas dataframe\n",
    "file_path = '03_chunked/' + state_name + '.parquet'\n",
    "data = pd.read_parquet(file_path)\n",
    "\n",
    "data['morphine_equivalent_g'] = data['CALC_BASE_WT_IN_GM']*data['MME_Conversion_Factor']\n",
    "\n",
    "print(data.sample(5))\n",
    "\n",
    "# Group the Texas DataFrame by specified columns and sum specified columns\n",
    "data_grouped = data.groupby(['BUYER_STATE', 'BUYER_COUNTY','TransactionYear'])\\\n",
    "               .sum(['morphine_equivalent_g'])\n",
    "data_grouped = data_grouped.reset_index()\n",
    "\n",
    "data_grouped.drop(columns=['TransactionMonth'], inplace=True)\n",
    "\n",
    "print(data_grouped.sample(5))\n",
    "\n",
    "# Save grouped data to parquet file\n",
    "file_name = '04_MME_WOFIPS/' + state_name + '.parquet'\n",
    "data_grouped.to_parquet(file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Check number of counties in the state\n",
    "file_path = '04_MME_WOFIPS/' + state_name + '.parquet'\n",
    "data = pd.read_parquet(file_path)\n",
    "data['BUYER_COUNTY'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of counties in the state\n",
    "file_path = 'old_data/ref_states/' + state_name + '.parquet'\n",
    "data = pd.read_parquet(file_path)\n",
    "data['BUYER_COUNTY'].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
