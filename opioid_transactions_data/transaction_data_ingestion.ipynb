{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zipfile36 import ZipFile\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths and parameters\n",
    "zip_file_path = '/workspaces/arcos_all_washpost.zip'\n",
    "csv_file_name = 'arcos_all_washpost.tsv'\n",
    "chunk_size = 25000\n",
    "keep_cols = ['BUYER_COUNTY', 'DOSAGE_UNIT', 'CALC_BASE_WT_IN_GM', 'BUYER_STATE', 'TRANSACTION_DATE', 'DRUG_NAME']\n",
    "filtered_data = []\n",
    "\n",
    "# Suppressing the SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Open the ZIP file containing the CSV file\n",
    "with ZipFile(zip_file_path, 'r') as zip_file:\n",
    "    # Check if the CSV file exists in the ZIP archive\n",
    "    if csv_file_name in zip_file.namelist():\n",
    "        # Open the CSV file from the ZIP archive\n",
    "        with zip_file.open(csv_file_name) as csv_file:\n",
    "            # Read the CSV file in chunks\n",
    "            csv_reader = pd.read_csv(csv_file, delimiter='\\t', chunksize=chunk_size, usecols=keep_cols, low_memory=False)\n",
    "            \n",
    "            # Iterate through chunks of the CSV file\n",
    "            for i, chunk in enumerate(csv_reader):\n",
    "                # Filter data for specific states\n",
    "                mod_chunk = chunk.loc[chunk['BUYER_STATE'].isin([\"FL\", \"TX\", \"WA\"])]\n",
    "                \n",
    "                # Convert TRANSACTION_DATE to string type\n",
    "                mod_chunk['TRANSACTION_DATE'] = mod_chunk['TRANSACTION_DATE'].astype(str)\n",
    "\n",
    "                try:\n",
    "                    # Split the date column and convert to integers\n",
    "                    mod_chunk[['TransactionYear', 'TransactionMonth', 'TransactionDay']] = mod_chunk['TRANSACTION_DATE'].str.split('-', expand=True).astype(int)\n",
    "                except Exception as e:\n",
    "                    # Handle the error by inserting NaN values for all three date columns\n",
    "                    mod_chunk[['TransactionYear', 'TransactionMonth', 'TransactionDay']] = pd.NA\n",
    "\n",
    "                # Drop the original 'TRANSACTION_DATE' and 'TransactionDay' columns\n",
    "                mod_chunk.drop(['TRANSACTION_DATE', 'TransactionDay'], axis=1, inplace=True)\n",
    "\n",
    "                # Keep data within the specified year range\n",
    "                append_chunk = mod_chunk[(mod_chunk['TransactionYear'] >= 2003) & (mod_chunk['TransactionYear'] <= 2015)]\n",
    "                \n",
    "                # Append the processed chunk to the list\n",
    "                filtered_data.append(append_chunk)\n",
    "\n",
    "        # Concatenate all processed chunks into a single DataFrame\n",
    "        selected_data = pd.concat(filtered_data, ignore_index=True)\n",
    "    else:\n",
    "        # Print an error message if the file is not found in the ZIP archive\n",
    "        print(f\"{csv_file_name} not found in the ZIP file.\")\n",
    "\n",
    "# Specify the path where you want to save the Parquet file\n",
    "parquet_file_path = 'opioid_transactions.parquet'\n",
    "\n",
    "# Convert the Pandas DataFrame to a PyArrow Table\n",
    "table = pa.Table.from_pandas(selected_data)\n",
    "\n",
    "# Write the Table to a Parquet file\n",
    "pq.write_table(table, parquet_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths and names\n",
    "zip_file_path = 'opioid_transactions.zip'\n",
    "pq_file_name = 'opioid_transactions.parquet'\n",
    "\n",
    "# Open the ZIP file containing the Parquet file\n",
    "with ZipFile(zip_file_path, 'r') as zip_file:\n",
    "    # Check if the Parquet file exists in the ZIP archive\n",
    "    if pq_file_name in zip_file.namelist():\n",
    "        # Open the Parquet file from the ZIP archive\n",
    "        with zip_file.open(pq_file_name) as pq_file:\n",
    "            # Read the Parquet file into a BytesIO buffer\n",
    "            # This is necessary because PyArrow requires a file-like object\n",
    "            pq_buffer = io.BytesIO(pq_file.read())\n",
    "\n",
    "            # Read the Parquet table from the buffer\n",
    "            # PyArrow reads the data into a Table format\n",
    "            table = pq.read_table(pq_buffer)\n",
    "\n",
    "            # Convert the PyArrow Table into a Pandas DataFrame\n",
    "            # This allows for easier manipulation and analysis of the data\n",
    "            df = table.to_pandas()\n",
    "    else:\n",
    "        # Print an error message if the Parquet file is not found in the ZIP archive\n",
    "        print(f\"{pq_file_name} not found in the ZIP file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read opioid transaction data into pandas dataframe\n",
    "file_path = \"opioid_transactions.parquet\"\n",
    "data = pd.read_parquet(file_path)\n",
    "\n",
    "# Subset for Florida\n",
    "florida = data[data['BUYER_STATE']=='FL']\n",
    "\n",
    "# Subset for Washington\n",
    "washington = data[data['BUYER_STATE']=='WA']\n",
    "\n",
    "# Subset for Texas\n",
    "texas = data[data['BUYER_STATE']=='TX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the Florida DataFrame by specified columns and sum specified columns\n",
    "florida_grouped = florida.groupby(['BUYER_STATE', 'BUYER_COUNTY', 'DRUG_NAME', 'TransactionYear', 'TransactionMonth'])\\\n",
    "               .sum(['CALC_BASE_WT_IN_GM', 'DOSAGE_UNIT'])\n",
    "florida_grouped = florida_grouped.reset_index()\n",
    "\n",
    "# Save Florida grouped data to parquet file\n",
    "file_name = 'florida.parquet'\n",
    "florida_grouped.to_parquet(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the Washington DataFrame by specified columns and sum specified columns\n",
    "washington_grouped = washington.groupby(['BUYER_STATE', 'BUYER_COUNTY', 'DRUG_NAME', 'TransactionYear', 'TransactionMonth'])\\\n",
    "               .sum(['CALC_BASE_WT_IN_GM', 'DOSAGE_UNIT'])\n",
    "washington_grouped = washington_grouped.reset_index()\n",
    "\n",
    "# Save Washington grouped data to parquet file\n",
    "file_name = 'washington.parquet'\n",
    "washington_grouped.to_parquet(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the Texas DataFrame by specified columns and sum specified columns\n",
    "texas_grouped = texas.groupby(['BUYER_STATE', 'BUYER_COUNTY', 'DRUG_NAME', 'TransactionYear', 'TransactionMonth'])\\\n",
    "               .sum(['CALC_BASE_WT_IN_GM', 'DOSAGE_UNIT'])\n",
    "texas_grouped = texas_grouped.reset_index()\n",
    "\n",
    "# Save Texas grouped data to parquet file\n",
    "file_name = 'texas.parquet'\n",
    "texas_grouped.to_parquet(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
